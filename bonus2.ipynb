{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Msrania/CS392Project-1/blob/main/bonus2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcG5jzxPqMmq"
      },
      "outputs": [],
      "source": [
        "# lib for linear algebra\n",
        "import numpy as np\n",
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "# data visualization\n",
        "# to see data\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import SGDOneClassSVM\n",
        "from sklearn import preprocessing\n",
        "# processing\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUXjRTgMqVlr",
        "outputId": "e907f1dc-af93-4535-f3ac-c17340bb1532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkCuyWvCqc6H",
        "outputId": "084fc2a8-e4b1-48c3-ba95-db25547be759"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['DB1', 'DB2', 'DB3', 'DB4']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir('/content/drive/MyDrive/Colab Notebooks/bonus/images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkAuMMQiqr5o"
      },
      "outputs": [],
      "source": [
        "DATADIR = r'/content/drive/MyDrive/Colab Notebooks/bonus/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ULdJ60qvFF",
        "outputId": "3a6172da-d874-46bb-c2bf-22d24cacc652"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['DB1', 'DB2', 'DB3', 'DB4']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CATEGORIES = os.listdir(DATADIR)\n",
        "CATEGORIES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvcceyVmAsZf"
      },
      "source": [
        "the data before applay any feature extraction methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcruoSG5AiGh"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    DATADIR = r'/content/drive/MyDrive/Colab Notebooks/bonus/images'\n",
        "    data = []\n",
        "    # loading training data\n",
        "    for category in CATEGORIES:\n",
        "        # create path to image of respective expression\n",
        "        path = os.path.join(DATADIR, category)\n",
        "        # get the classification  for each expression\n",
        "        class_num = CATEGORIES.index(category)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_array = cv2.imread(os.path.join(path, img), 0)\n",
        "            data.append([img_array, class_num])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hooULLX6AlPV",
        "outputId": "e2adfc5c-50ef-401b-9198-b021f0b192af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 80/80 [00:21<00:00,  3.72it/s]\n",
            "100%|██████████| 80/80 [00:16<00:00,  4.85it/s]\n",
            "100%|██████████| 80/80 [00:19<00:00,  4.07it/s]\n",
            "  0%|          | 0/80 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "data = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s31MKPWaAmnz"
      },
      "outputs": [],
      "source": [
        "L = 6\n",
        "W = 4\n",
        "fig, axes = plt.subplots(L, W, figsize = (10,10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(0, L * W):\n",
        "    sample = random.choice(data)\n",
        "    axes[i].set_title(\"lable = \"+str(CATEGORIES[sample[1]]))\n",
        "    axes[i].imshow(sample[0], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.subplots_adjust(wspace=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkR0bWNSrR3d"
      },
      "source": [
        "it's a good idea to apply the Harris corner detector on the entire image dataset before splitting it into training and testing sets. This ensures that the same set of corner features are used for both training and testing, which is important for evaluating the performance of the SVM model on unseen data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6EzUck9qzmh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def load_data():\n",
        "    DATADIR = r'/content/drive/MyDrive/Colab Notebooks/bonus/images'\n",
        "    data = []\n",
        "    # loading training data\n",
        "    for category in CATEGORIES:\n",
        "        # create path to image of respective expression\n",
        "        path = os.path.join(DATADIR, category)\n",
        "        # get the classification  for each expression\n",
        "        class_num = CATEGORIES.index(category)\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_path = os.path.join(path, img)\n",
        "            img_array = cv2.imread(img_path, 0)\n",
        "            # Resize the image to a consistent size\n",
        "            img_array = cv2.resize(img_array, (100, 100))\n",
        "            # Apply the Harris corner detector to the image\n",
        "            block_size = 2\n",
        "            ksize = 3\n",
        "            k = 0.04\n",
        "            dst = cv2.cornerHarris(img_array, block_size, ksize, k)\n",
        "            dst_norm = np.empty_like(dst)\n",
        "            cv2.normalize(dst, dst_norm, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
        "            dst_norm = np.uint8(dst_norm)\n",
        "            # Add the corner response image and the class label to the data list\n",
        "            #store the resulting corner response image and the corresponding label as a tuple of the form [dst_norm, class_num]\n",
        "            feature_vector = dst_norm.flatten()\n",
        "            data.append([dst_norm, class_num])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lnDkHjaq28R"
      },
      "outputs": [],
      "source": [
        "data = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40kP9dr_q59p"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xvBPy1aq8TE"
      },
      "outputs": [],
      "source": [
        "L = 6\n",
        "W = 4\n",
        "fig, axes = plt.subplots(L, W, figsize = (10,10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(0, L * W):\n",
        "    sample = random.choice(data)\n",
        "    axes[i].set_title(\"lable = \"+str(CATEGORIES[sample[1]]))\n",
        "    axes[i].imshow(sample[0], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.subplots_adjust(wspace=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOJsrvVM_NbJ"
      },
      "outputs": [],
      "source": [
        "# Convert the data list to numpy arrays for use in machine learning\n",
        "#To use features vector as input to a machine learning algorithm, we need to flatten the 2D array of the corner response image into a 1D feature vector\n",
        "X = np.array([x[0].flatten() for x in data])\n",
        "y = np.array([x[1] for x in data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAG7mB6h_TCG"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYjMAK-v_keP"
      },
      "outputs": [],
      "source": [
        "# Train an SVM model on the preprocessed data\n",
        "clf = SVC(kernel='linear', random_state=42).fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjW1FqNM_oPS"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2lYS9TvK2jP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHxodp1cFJtr"
      },
      "source": [
        "# apply Canny edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRqz1sCELWEf"
      },
      "source": [
        "apply canny edge by load_data function, we can modify the function to preprocess each image using the Canny edge detection algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RFW3dtqrfAaM",
        "outputId": "ba8ce81c-d66b-44e1-b221-315a23bfea8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYoyjYzRPtm9"
      },
      "outputs": [],
      "source": [
        "img_array = cv2.imread('/content/drive/MyDrive/Colab Notebooks/bonus/images/DB1/101_1.tif', 0)\n",
        "edges = cv2.Canny(img_array, 100, 200)\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].imshow(img_array, cmap='gray')\n",
        "axs[0].set_title('Original Image')\n",
        "axs[1].imshow(edges, cmap='gray')\n",
        "axs[1].set_title('Edges')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6PyT25vl97W"
      },
      "outputs": [],
      "source": [
        "# Define the function to load the data\n",
        "def load_data():\n",
        "    DATADIR = r'/content/drive/MyDrive/Colab Notebooks/bonus/images'\n",
        "    data = []\n",
        "    # loading training data\n",
        "    for category in CATEGORIES:\n",
        "        # create path to image of respective expression\n",
        "        path = os.path.join(DATADIR, category)\n",
        "        # get the classification  for each expression\n",
        "        class_num = CATEGORIES.index(category)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_path = os.path.join(path,img)\n",
        "            img_array = cv2.imread(img_path, 0)\n",
        "\n",
        "            # Check if the loaded image is empty\n",
        "            if img_array is None:\n",
        "                continue\n",
        "\n",
        "            # Resize the image to a consistent size\n",
        "            img_array = cv2.resize(img_array, (100, 100))\n",
        "\n",
        "            # Apply Canny edge detection to the image\n",
        "            edges = cv2.Canny(img_array, 100, 100)\n",
        "\n",
        "\n",
        "# Display the original image and the edge image side by side\n",
        "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            ax[0].imshow(img_array, cmap='gray')\n",
        "            ax[0].set_title('Original Image')\n",
        "            ax[1].imshow(edges, cmap='gray')\n",
        "            ax[1].set_title('Edge Image')\n",
        "            plt.show()\n",
        "\n",
        "            # Compute the feature vector for the image\n",
        "            feature_vector_img = edges.flatten()\n",
        "            data.append([feature_vector_img, class_num])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Egcwzj5-MK4"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Es1bg7nAWgc"
      },
      "outputs": [],
      "source": [
        "# Extract the feature vectors and labels from the data\n",
        "X = np.array([x[0].flatten() for x in data])\n",
        "y = np.array([x[1] for x in data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBiLd_O_C3IJ"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRq6MT5nCB4p"
      },
      "outputs": [],
      "source": [
        "# Train an SVM classifier\n",
        "from sklearn.svm import SVC\n",
        "# Train an SVM classifier\n",
        "# Train an SVM model on the preprocessed data\n",
        "clf = SVC(kernel='linear', random_state=42).fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t5OquwEfjMn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.path.exists(DATADIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZw-vD5BHBio"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJge-AZ1HRzK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ4u0DURHVKh"
      },
      "source": [
        "# apply SIFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3nEtM5vHmmX"
      },
      "outputs": [],
      "source": [
        "# Define the function to load the data\n",
        "def load_data():\n",
        "    DATADIR = r'/content/drive/MyDrive/Colab Notebooks/bonus/images'\n",
        "    data = []\n",
        "    # loading training data\n",
        "    for category in CATEGORIES:\n",
        "        # create path to image of respective expression\n",
        "        path = os.path.join(DATADIR, category)\n",
        "        # get the classification  for each expression\n",
        "        class_num = CATEGORIES.index(category)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_path = os.path.join(path,img)\n",
        "            img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Check if the loaded image is empty\n",
        "            if img_array is None:\n",
        "                continue\n",
        "\n",
        "            # Resize the image to a consistent size\n",
        "            img_array = cv2.resize(img_array, (100, 100))\n",
        "\n",
        "            # Extract SIFT features from the image\n",
        "            sift = cv2.xfeatures2d.SIFT_create()\n",
        "            keypoints, descriptors = sift.detectAndCompute(img_array, None)\n",
        "\n",
        "            # Visualize the keypoints detected\n",
        "            img_kp = cv2.drawKeypoints(img_array, keypoints, None)\n",
        "            plt.imshow(img_kp)\n",
        "            plt.title('Keypoints for image {}'.format(img_path))\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "            # Compute the feature vector for the image\n",
        "            feature_vector_img = descriptors.flatten()\n",
        "            data.append([feature_vector_img, class_num])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUcHjTt-IpQ6"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEB8MnaKJclh"
      },
      "outputs": [],
      "source": [
        "# Extract the feature vectors and labels from the data\n",
        "max_len = max([len(x[0]) for x in data])\n",
        "X = np.array([np.pad(x[0], (0, max_len - len(x[0])), 'constant') for x in data])\n",
        "y = np.array([x[1] for x in data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9guHURMGJkC1"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQHsFG7qJoXc"
      },
      "outputs": [],
      "source": [
        "# Train an SVM classifier\n",
        "from sklearn.svm import SVC\n",
        "# Train an SVM classifier\n",
        "# Train an SVM model on the preprocessed data\n",
        "clf = SVC(kernel='linear', random_state=42).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3wBONxQKc74"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDLhr47YKhx6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g24Vz9GuKqcl"
      },
      "source": [
        "accuracy, precision, and F1 after using the three features as a single feature vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQrU_WBgMFjp"
      },
      "outputs": [],
      "source": [
        "# Define the function to load the data\n",
        "def load_data():\n",
        "    DATADIR = r'/content/drive/MyDrive/Colab Notebooks/bonus/images'\n",
        "    data = []\n",
        "    # loading training data\n",
        "    for category in CATEGORIES:\n",
        "        # create path to image of respective expression\n",
        "        path = os.path.join(DATADIR, category)\n",
        "        # get the classification  for each expression\n",
        "        class_num = CATEGORIES.index(category)\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_path = os.path.join(path,img)\n",
        "            img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Check if the loaded image is empty\n",
        "            if img_array is None:\n",
        "                continue\n",
        "            # Resize the image to a consistent size\n",
        "            img_array = cv2.resize(img_array, (100, 100))\n",
        "            # Extract Harris features from the image\n",
        "            harris = cv2.cornerHarris(img_array, 2, 3, 0.04)\n",
        "            harris = cv2.normalize(harris, None, 0, 1, cv2.NORM_MINMAX)\n",
        "            harris = harris.reshape(-1)\n",
        "            # Extract Canny edges from the image\n",
        "            canny = cv2.Canny(img_array, 100, 200)\n",
        "            canny = canny.reshape(-1)\n",
        "            # Extract SIFT features from the image\n",
        "            sift = cv2.xfeatures2d.SIFT_create()\n",
        "            keypoints, descriptors = sift.detectAndCompute(img_array, None)\n",
        "            sift = descriptors.flatten()\n",
        "            # Combine the feature vectors into a single feature vector\n",
        "            feature_vector = np.concatenate((harris, canny, sift))\n",
        "            # Visualize the combined feature vector\n",
        "            plt.plot(feature_vector)\n",
        "            plt.title('Combined feature vector for image {}'.format(img_path))\n",
        "            plt.show()\n",
        "            # Add the feature vector and label to the data list\n",
        "            data.append([feature_vector, class_num])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44idT3AgMIag"
      },
      "outputs": [],
      "source": [
        " data = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iitCiWRCNBSO"
      },
      "outputs": [],
      "source": [
        "# Extract the feature vectors and labels from the data\n",
        "max_len = max([len(x[0]) for x in data])\n",
        "X = np.array([np.pad(x[0], (0, max_len - len(x[0])), 'constant') for x in data])\n",
        "y = np.array([x[1] for x in data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlVcB1BKNVqv"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4791FZpMvA_"
      },
      "outputs": [],
      "source": [
        "# Train an SVM classifier\n",
        "from sklearn.svm import SVC\n",
        "# Train an SVM classifier\n",
        "# Train an SVM model on the preprocessed data\n",
        "clf = SVC(kernel='linear', random_state=42).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvdla9ylNv7l"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred, average='weighted')\n",
        "rec = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(acc*100))\n",
        "print(\"Precision: {:.2f}%\".format(prec*100))\n",
        "print(\"Recall: {:.2f}%\".format(rec*100))\n",
        "print(\"F1 Score: {:.2f}%\".format(f1*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}